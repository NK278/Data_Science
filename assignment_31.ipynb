{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e7ebc5-c6cd-4e1c-8617-4a2fbad75e62",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e244367-2990-42eb-8140-20dcf0f4e0ea",
   "metadata": {},
   "source": [
    "Ans: **Min-Max scaling**, also known as min-max normalization or feature scaling, is a data preprocessing technique used to transform numerical features to a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to ensure that all features have the same scale, preventing features with larger magnitudes from dominating the learning process in certain machine learning algorithms.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature value.\n",
    "- \\( X_{\\text{min}} \\) is the minimum value of the feature in the dataset.\n",
    "- \\( X_{\\text{max}} \\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a dataset with a feature, \"Income,\" which has values ranging from $20,000 to $100,000. We want to scale this feature using Min-Max scaling.\n",
    "\n",
    "1. **Original Data (Income):**\n",
    "   - \\( X_{\\text{min}} = $20,000 \\)\n",
    "   - \\( X_{\\text{max}} = $100,000 \\)\n",
    "\n",
    "2. **Min-Max Scaling Formula:**\n",
    "   \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "3. **Applying Min-Max Scaling:**\n",
    "   - For \\( X = $30,000 \\):\n",
    "     \\[ X_{\\text{scaled}} = \\frac{30,000 - 20,000}{100,000 - 20,000} = \\frac{10,000}{80,000} = 0.125 \\]\n",
    "\n",
    "   - For \\( X = $70,000 \\):\n",
    "     \\[ X_{\\text{scaled}} = \\frac{70,000 - 20,000}{100,000 - 20,000} = \\frac{50,000}{80,000} = 0.625 \\]\n",
    "\n",
    "   - For \\( X = $90,000 \\):\n",
    "     \\[ X_{\\text{scaled}} = \\frac{90,000 - 20,000}{100,000 - 20,000} = \\frac{70,000}{80,000} = 0.875 \\]\n",
    "\n",
    "4. **Scaled Data (Income):**\n",
    "   - \\( X_{\\text{scaled}}(30,000) = 0.125 \\)\n",
    "   - \\( X_{\\text{scaled}}(70,000) = 0.625 \\)\n",
    "   - \\( X_{\\text{scaled}}(90,000) = 0.875 \\)\n",
    "\n",
    "Now, the \"Income\" feature has been scaled to a range between 0 and 1, making it suitable for use in machine learning models that are sensitive to the scale of input features.\n",
    "\n",
    "**Benefits of Min-Max Scaling:**\n",
    "- Ensures that all features contribute equally to the learning process, preventing one feature from dominating due to a larger magnitude.\n",
    "- Suitable for algorithms that rely on distance metrics, such as k-nearest neighbors and clustering algorithms.\n",
    "- Maintains the shape of the original distribution while transforming the scale of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36f554-24ad-4208-90d3-7a0bf1cb02f8",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3fa05-0139-42fa-ab4d-bbfdd1e6f804",
   "metadata": {},
   "source": [
    "Ans: **Unit Vector Scaling**, also known as **Normalization** or **L2 normalization**, is a feature scaling technique that scales each feature such that the Euclidean norm (L2 norm) of the feature vector becomes 1. This technique is particularly useful when the magnitude of features is important, but the direction of the vector matters more.\n",
    "\n",
    "The formula for unit vector scaling is as follows:\n",
    "\n",
    "\\[ X_{\\text{normalized}} = \\frac{X}{\\|X\\|_2} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature vector.\n",
    "- \\( \\|X\\|_2 \\) is the Euclidean norm of the feature vector, calculated as \\(\\sqrt{\\sum_{i=1}^{n} x_i^2}\\), where \\(n\\) is the number of features.\n",
    "\n",
    "Unit Vector Scaling transforms the original feature vector into a unit vector without changing the direction of the vector, only its magnitude.\n",
    "\n",
    "Now, let's compare Unit Vector Scaling with Min-Max Scaling and provide an example:\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a dataset with two features, \"Length\" and \"Width,\" and we want to scale these features using both Min-Max scaling and Unit Vector Scaling.\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Feature \"Length\" ranges from 2 to 8.\n",
    "   - Feature \"Width\" ranges from 1 to 5.\n",
    "\n",
    "2. **Min-Max Scaling:**\n",
    "   - Apply Min-Max scaling separately to each feature using the formula mentioned earlier.\n",
    "\n",
    "3. **Unit Vector Scaling:**\n",
    "   - For each data point (sample), calculate the Euclidean norm (\\(\\|X\\|_2\\)) of the feature vector and then divide each feature by this norm.\n",
    "\n",
    "   \\[ X_{\\text{normalized}} = \\frac{X}{\\sqrt{\\text{Length}^2 + \\text{Width}^2}} \\]\n",
    "\n",
    "   - This ensures that the resulting feature vector has a Euclidean norm of 1.\n",
    "\n",
    "4. **Comparison:**\n",
    "\n",
    "   | Sample | Length | Width | Min-Max Scaled \"Length\" | Min-Max Scaled \"Width\" | Unit Vector Scaled \"Length\" | Unit Vector Scaled \"Width\" |\n",
    "   |--------|--------|-------|-------------------------|------------------------|-----------------------------|----------------------------|\n",
    "   | 1      | 2      | 1     | 0                       | 0                      | 0.5547                      | 0.8321                     |\n",
    "   | 2      | 5      | 3     | 0.5                     | 0.5                    | 0.7071                      | 0.7071                     |\n",
    "   | 3      | 8      | 5     | 1                       | 1                      | 0.7454                      | 0.6667                     |\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "- **Min-Max Scaling:**\n",
    "  - Adjusts each feature to a specific range (e.g., between 0 and 1).\n",
    "  - Preserves the relative distances between data points in the original feature space.\n",
    "\n",
    "- **Unit Vector Scaling:**\n",
    "  - Adjusts the entire feature vector so that its Euclidean norm becomes 1.\n",
    "  - Preserves the direction of the feature vector but adjusts its magnitude.\n",
    "\n",
    "**Use Cases:**\n",
    "- Min-Max Scaling is suitable when the scale of features is critical, and the relative distances between data points matter.\n",
    "- Unit Vector Scaling is suitable when the direction of the feature vector is more important, and we want to emphasize the contribution of each feature in the same direction.\n",
    "\n",
    "Both scaling techniques are valuable depending on the specific requirements of the machine learning algorithm and the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b40729-5d59-4798-beb2-a4cbd464af7c",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcded2-8adc-41e1-9b49-c654370912ce",
   "metadata": {},
   "source": [
    "Ans: **Principal Component Analysis (PCA)** is a dimensionality reduction technique widely used in machine learning and data analysis. The main goal of PCA is to transform high-dimensional data into a lower-dimensional space while retaining as much of the original variance as possible. It achieves this by identifying the principal components, which are linear combinations of the original features that capture the most significant sources of variation in the data.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "1. **Standardize the Data:**\n",
    "   - If the features have different scales, it's essential to standardize them (subtract the mean and divide by the standard deviation) to ensure that each feature contributes equally to the analysis.\n",
    "\n",
    "2. **Compute the Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized data. The covariance matrix provides information about the relationships between different features.\n",
    "\n",
    "3. **Compute Eigenvectors and Eigenvalues:**\n",
    "   - Determine the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions (principal components) in which the data varies the most, and the corresponding eigenvalues indicate the magnitude of the variance in those directions.\n",
    "\n",
    "4. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvector with the highest eigenvalue represents the principal component with the most significant variance.\n",
    "\n",
    "5. **Choose the Number of Principal Components:**\n",
    "   - Decide on the number of principal components to retain. This decision can be based on the explained variance, where a higher percentage of explained variance indicates better retention of information.\n",
    "\n",
    "6. **Project Data onto Principal Components:**\n",
    "   - Use the selected principal components to transform the original data into a lower-dimensional space. This new set of features, called principal components, captures the most important information from the original data.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's consider a dataset with two features, \"Height\" and \"Weight,\" and we want to apply PCA to reduce the dimensionality of the data.\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Feature \"Height\" ranges from 150 to 180 (in cm).\n",
    "   - Feature \"Weight\" ranges from 50 to 90 (in kg).\n",
    "\n",
    "2. **Standardize the Data:**\n",
    "   - Subtract the mean and divide by the standard deviation for each feature.\n",
    "\n",
    "3. **Compute Covariance Matrix:**\n",
    "   - Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "4. **Compute Eigenvectors and Eigenvalues:**\n",
    "   - Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "5. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "\n",
    "6. **Choose the Number of Principal Components:**\n",
    "   - Decide to retain, for example, the top 1 principal component.\n",
    "\n",
    "7. **Project Data onto Principal Components:**\n",
    "   - Multiply the original data by the selected eigenvector to obtain the lower-dimensional representation.\n",
    "\n",
    "PCA effectively reduces the dimensionality of the dataset by projecting it onto a smaller number of principal components, which capture the most significant variations in the data. This reduction in dimensionality can be beneficial for visualization, computational efficiency, and mitigating the curse of dimensionality in certain machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a3455-50d0-42dc-834e-331c79b7f0cd",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114209c-765d-41b3-8c1e-730e0776e4fb",
   "metadata": {},
   "source": [
    "Ans: **Principal Component Analysis (PCA)** can be considered a form of **feature extraction**. Feature extraction involves transforming the original features of a dataset into a new set of features, often of reduced dimensionality, while retaining as much relevant information as possible. PCA achieves this by identifying the principal components, which are linear combinations of the original features that capture the most significant sources of variation in the data.\n",
    "\n",
    "### Relationship between PCA and Feature Extraction:\n",
    "\n",
    "1. **Dimensionality Reduction:**\n",
    "   - PCA is commonly used for dimensionality reduction by projecting the original features onto a lower-dimensional subspace defined by the principal components.\n",
    "\n",
    "2. **Information Compression:**\n",
    "   - The principal components are chosen in such a way that they capture the maximum variance in the data. By selecting a subset of the principal components, PCA allows for information compression, summarizing the essential patterns in the data with fewer features.\n",
    "\n",
    "3. **Reducing Redundancy:**\n",
    "   - PCA tends to reduce the redundancy in the original features by emphasizing the directions in which the data varies the most. This reduction in redundancy can lead to a more concise representation of the data.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a dataset with three features: \"Temperature,\" \"Humidity,\" and \"Air Pressure.\" We want to use PCA for feature extraction to reduce the dimensionality of the data.\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Feature \"Temperature\" ranges from 20 to 30 degrees Celsius.\n",
    "   - Feature \"Humidity\" ranges from 30% to 70%.\n",
    "   - Feature \"Air Pressure\" ranges from 1000 to 1020 hPa.\n",
    "\n",
    "2. **Standardize the Data:**\n",
    "   - Subtract the mean and divide by the standard deviation for each feature.\n",
    "\n",
    "3. **Compute Covariance Matrix:**\n",
    "   - Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "4. **Compute Eigenvectors and Eigenvalues:**\n",
    "   - Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "5. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "\n",
    "6. **Choose the Number of Principal Components:**\n",
    "   - Decide to retain, for example, the top 2 principal components.\n",
    "\n",
    "7. **Project Data onto Principal Components:**\n",
    "   - Multiply the original data by the selected eigenvectors to obtain a lower-dimensional representation.\n",
    "\n",
    "The resulting transformed dataset will have two new features (principal components) that capture the most significant variations in the original data. These principal components can be considered as the extracted features that represent the essential patterns in the dataset.\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    "| Temperature | Humidity | Air Pressure |\n",
    "|-------------|----------|--------------|\n",
    "| 20          | 30       | 1000         |\n",
    "| 25          | 50       | 1010         |\n",
    "| 30          | 70       | 1020         |\n",
    "\n",
    "Transformed Data (using top 2 principal components):\n",
    "| PC1        | PC2         |\n",
    "|------------|-------------|\n",
    "| -1.73      | -0.63       |\n",
    "| 0          | 0           |\n",
    "| 1.73       | 0.63        |\n",
    "```\n",
    "\n",
    "In this example, the original three features are compressed into two principal components (PC1 and PC2), achieving feature extraction through PCA. The transformed data captures the most important information in the original dataset in a reduced-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d7e94-4dfa-4496-96be-f8e5fa840afe",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d11d1-9c6b-4fac-b69b-0026c1835104",
   "metadata": {},
   "source": [
    "Ans: In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data, ensuring that the features are on a consistent scale. This is important because it prevents features with larger magnitudes from disproportionately influencing the recommendation algorithm. Here's how you could use Min-Max scaling for preprocessing:\n",
    "\n",
    "### Steps for Using Min-Max Scaling:\n",
    "\n",
    "1. **Identify Features:**\n",
    "   - Identify the features in your dataset that need to be scaled. In this case, features such as \"price,\" \"rating,\" and \"delivery time\" are relevant.\n",
    "\n",
    "2. **Standardize the Data:**\n",
    "   - For each feature, subtract the minimum value and divide by the range (difference between the maximum and minimum values). The formula for Min-Max scaling is:\n",
    "     \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "3. **Apply Min-Max Scaling:**\n",
    "   - Apply the Min-Max scaling transformation to each feature individually. This will ensure that each feature is scaled to a range between 0 and 1.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's consider a simplified subset of the dataset with three features: \"price,\" \"rating,\" and \"delivery time.\"\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    "| Price | Rating | Delivery Time |\n",
    "|-------|--------|---------------|\n",
    "| $10   | 4.5    | 30 minutes    |\n",
    "| $20   | 3.8    | 45 minutes    |\n",
    "| $15   | 4.2    | 35 minutes    |\n",
    "```\n",
    "\n",
    "1. **Identify Features:**\n",
    "   - Features to be scaled: \"Price,\" \"Rating,\" and \"Delivery Time.\"\n",
    "\n",
    "2. **Standardize the Data:**\n",
    "   - Convert \"Price\" to numerical format (remove the dollar sign).\n",
    "   - Convert \"Delivery Time\" to numerical format (e.g., minutes).\n",
    "   - \"Rating\" is already numerical.\n",
    "\n",
    "3. **Apply Min-Max Scaling:**\n",
    "   - For each feature, apply the Min-Max scaling transformation.\n",
    "\n",
    "     \\[ \\text{Scaled Price} = \\frac{\\text{Price} - \\text{Min(Price)}}{\\text{Max(Price)} - \\text{Min(Price)}} \\]\n",
    "\n",
    "     \\[ \\text{Scaled Rating} = \\frac{\\text{Rating} - \\text{Min(Rating)}}{\\text{Max(Rating)} - \\text{Min(Rating)}} \\]\n",
    "\n",
    "     \\[ \\text{Scaled Delivery Time} = \\frac{\\text{Delivery Time} - \\text{Min(Delivery Time)}}{\\text{Max(Delivery Time)} - \\text{Min(Delivery Time)}} \\]\n",
    "\n",
    "   - The resulting scaled features will have values between 0 and 1.\n",
    "\n",
    "The scaled data can then be used as input for the recommendation system, ensuring that each feature contributes proportionally to the recommendation algorithm, regardless of its original scale. This preprocessing step helps in achieving a fair and consistent representation of the features in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d1b76-e513-4a65-b472-b1e069a0284d",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d22ac5-47c3-43c6-936b-bc946fcb5998",
   "metadata": {},
   "source": [
    "Ans: In the context of building a model to predict stock prices with a dataset containing numerous features, Principal Component Analysis (PCA) can be a valuable tool for dimensionality reduction. The primary goal is to transform the high-dimensional dataset into a lower-dimensional space while retaining the most significant sources of variation. Here's how you could use PCA for this purpose:\n",
    "\n",
    "### Steps for Using PCA in Stock Price Prediction:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Standardize the features: Ensure that all features are on a consistent scale by subtracting the mean and dividing by the standard deviation. This step is crucial for PCA.\n",
    "\n",
    "2. **Apply PCA:**\n",
    "   - Compute the covariance matrix of the standardized data.\n",
    "\n",
    "   - Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "   - Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue represents the principal component with the most significant variance.\n",
    "\n",
    "   - Choose the number of principal components to retain. This decision can be based on the explained variance, where a higher percentage of explained variance indicates better retention of information.\n",
    "\n",
    "   - Project the original data onto the selected principal components to obtain a lower-dimensional representation.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's consider a simplified subset of the dataset with various financial features and market trends for a company:\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    "| Feature1 | Feature2 | ... | FeatureN |\n",
    "|-----------|-----------|-----|----------|\n",
    "| ...       | ...       | ... | ...      |\n",
    "```\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Standardize each feature to have zero mean and unit variance.\n",
    "\n",
    "2. **Apply PCA:**\n",
    "   - Compute the covariance matrix of the standardized data.\n",
    "\n",
    "   - Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "   - Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "\n",
    "   - Choose the number of principal components to retain. This could be determined by the desired level of explained variance.\n",
    "\n",
    "   - Project the original data onto the selected principal components.\n",
    "\n",
    "   - The resulting dataset will have fewer features (principal components) that capture the most significant variations in the original data.\n",
    "\n",
    "The reduced-dimensional dataset obtained from PCA can then be used as input for training a stock price prediction model. By reducing the dimensionality, you achieve several benefits, including a potential reduction in noise, improved computational efficiency, and potentially better generalization performance of the model. It's important to experiment with different numbers of principal components and monitor the explained variance to find the right balance between dimensionality reduction and information retention for your specific prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1b864-86e2-4b56-939a-6e0ee956c066",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d16372e-f9da-4dea-be73-50b04a13e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Given dataset\n",
    "original_values = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Min-Max scaling function\n",
    "def min_max_scaling(x):\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    x_scaled = (x - x_min) / (x_max - x_min)\n",
    "    return x_scaled\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_values = min_max_scaling(original_values)\n",
    "\n",
    "# Print the original and scaled values\n",
    "# print(\"Original Values:\", original_values)\n",
    "# print(\"Scaled Values:\", scaled_values)\n",
    "df=pd.DataFrame(original_values,columns=['data'])\n",
    "df_scaled=pd.DataFrame(scaled_values,columns=['scaled_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c02fb30-beb2-4cce-9f8a-119a31ff0555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data\n",
       "0     1\n",
       "1     5\n",
       "2    10\n",
       "3    15\n",
       "4    20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23759fc-688b-4723-8302-1ab563930a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_data\n",
       "0     0.000000\n",
       "1     0.210526\n",
       "2     0.473684\n",
       "3     0.736842\n",
       "4     1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e0f2b-c074-4f51-b8c9-9f5d268ffd26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
