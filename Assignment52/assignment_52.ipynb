{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9001f2e-fb8b-47f7-9231-5031f52c5daa",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a5c93-9a6e-4916-96c2-a6c67a1eea98",
   "metadata": {},
   "source": [
    "Ans: Polynomial functions and kernel functions in machine learning algorithms are both used to transform input data into a higher-dimensional space, often to make the data more separable or to capture more complex patterns. While they serve similar purposes, they operate differently and are applied in distinct contexts:\n",
    "\n",
    "1. **Polynomial Functions**:\n",
    "   - Polynomial functions are mathematical functions of the form $  f(x) = a_n x^n + a_{n-1} x^{n-1} + \\ldots + a_1 x + a_0 $ where $ x $ is the input variable, and $ a_n, a_{n-1}, \\ldots, a_1, a_0 $ are coefficients.\n",
    "   - In machine learning, polynomial functions are often used as basis functions in polynomial regression models or as feature transformations in polynomial kernel methods.\n",
    "   - Polynomial regression fits a polynomial curve to the data by minimizing the error between the actual and predicted values.\n",
    "   - Polynomial kernel methods, such as Support Vector Machines (SVMs) with polynomial kernels, use polynomial functions to map input data into a higher-dimensional space, where the data may become linearly separable.\n",
    "\n",
    "2. **Kernel Functions**:\n",
    "   - Kernel functions are used in various machine learning algorithms, such as Support Vector Machines (SVMs), kernelized versions of Principal Component Analysis (PCA), and kernelized versions of clustering algorithms.\n",
    "   - A kernel function computes the inner product of the input vectors in a high-dimensional feature space, without explicitly mapping the data into that space.\n",
    "   - Common kernel functions include linear, polynomial, Gaussian (RBF), sigmoid, and more.\n",
    "   - Kernel functions enable algorithms to operate efficiently in high-dimensional spaces by implicitly computing the dot products between transformed feature vectors.\n",
    "\n",
    "**Relationship**:\n",
    "- Polynomial functions can be used as kernel functions in machine learning algorithms.\n",
    "- In SVMs, for example, polynomial kernels utilize polynomial functions to implicitly map data into a higher-dimensional space, making it easier to find a separating hyperplane.\n",
    "- By applying a polynomial kernel, SVMs can capture complex relationships between data points that might not be linearly separable in the original feature space.\n",
    "- Thus, while polynomial functions and kernel functions are distinct concepts, they are often intertwined in the context of machine learning algorithms like SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2409292-b18c-4073-9cc2-1d45be48c61b",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3997a7ce-5e9f-4729-8c85-7ee3dc25713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset (or any other dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aabd53-c850-493a-88dd-cee339359f1a",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1485b9-2e0c-4b5d-9289-f24199489de6",
   "metadata": {},
   "source": [
    "Ans: In Support Vector Regression (SVR), epsilon (\\(\\epsilon\\)) is a parameter that controls the width of the margin of the support vector regression model. The margin is the region around the predicted function within which errors are not penalized.\n",
    "\n",
    "When you increase the value of epsilon in SVR:\n",
    "\n",
    "1. **Wider Margin**:\n",
    "   - Increasing epsilon expands the width of the margin around the predicted function. It allows more data points to fall within the margin without contributing to the loss function.\n",
    "   - A wider margin implies that the model is more tolerant of errors or deviations from the predicted function.\n",
    "\n",
    "2. **Impact on Support Vectors**:\n",
    "   - Support vectors are the data points that lie on the margin boundary or within the margin.\n",
    "   - As you increase epsilon, more data points may fall within the wider margin without becoming support vectors.\n",
    "   - Conversely, if epsilon is small, fewer data points can fit within the narrower margin, thus potentially increasing the number of support vectors.\n",
    "\n",
    "3. **Complexity of the Model**:\n",
    "   - Increasing epsilon tends to simplify the SVR model by allowing a larger margin and fewer support vectors.\n",
    "   - A simpler model may generalize better to unseen data and may be less prone to overfitting, especially if the training data contains noise or outliers.\n",
    "\n",
    "4. **Trade-off with Accuracy**:\n",
    "   - While increasing epsilon can simplify the model and make it more robust, it may also reduce the accuracy of predictions, particularly if the dataset contains important patterns or features near the margin.\n",
    "   - It's essential to strike a balance between the margin width (controlled by epsilon) and the model's predictive accuracy.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR tends to widen the margin, potentially reducing the number of support vectors and simplifying the model. However, this trade-off should be carefully considered based on the dataset characteristics, the desired level of model complexity, and the need for predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b06ef-c853-4c53-8a4f-bcdcc4a41080",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dce6a5-c620-4528-9600-d3e75be58942",
   "metadata": {},
   "source": [
    "Ans: Support Vector Regression (SVR) performance is heavily influenced by several key parameters: the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Let's delve into each parameter's function and how adjusting its value can impact SVR performance:\n",
    "\n",
    "1. **Choice of Kernel Function**:\n",
    "   - SVR can use different kernel functions such as linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - The choice of kernel function determines the mapping of input features into a higher-dimensional space where SVR attempts to find a linear relationship.\n",
    "   - Depending on the dataset and the underlying patterns, different kernel functions may perform better.\n",
    "   - For example, RBF kernel tends to capture complex nonlinear relationships, while linear kernel works well for linearly separable data.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - The C parameter controls the trade-off between maximizing the margin and minimizing the training error.\n",
    "   - A smaller C value encourages a wider margin, allowing more training points to be classified correctly but potentially leading to more margin violations.\n",
    "   - Conversely, a larger C value penalizes margin violations more heavily, resulting in a narrower margin and potentially better fitting to the training data.\n",
    "   - Increasing C may lead to overfitting, especially if the dataset contains noise or outliers.\n",
    "\n",
    "3. **Epsilon Parameter**:\n",
    "   - Epsilon $ \\epsilon $ determines the width of the epsilon-insensitive tube around the regression line, within which errors are not penalized.\n",
    "   - Larger values of epsilon result in a wider tube, allowing more points to be within the margin of tolerance.\n",
    "   - A smaller epsilon implies a narrower tolerance, leading to a more sensitive model to errors.\n",
    "   - Increasing epsilon can lead to a more robust model, but it may sacrifice accuracy if the dataset contains important patterns near the margin.\n",
    "\n",
    "4. **Gamma Parameter**:\n",
    "   - Gamma $ \\gamma $ is a parameter specific to kernel functions like RBF. It defines the influence of a single training example, with low values meaning far and high values meaning close.\n",
    "   - A smaller gamma value makes the decision boundary smoother, potentially underfitting the model.\n",
    "   - Conversely, a larger gamma value makes the boundary more complex, potentially leading to overfitting.\n",
    "   - The choice of gamma affects the flexibility of the decision boundary and the generalization ability of the model.\n",
    "\n",
    "Here are some examples of scenarios where you might want to adjust the parameter values:\n",
    "\n",
    "- **Increase C**: When you have confidence in your training data and want to minimize training errors, but be cautious of overfitting.\n",
    "- **Increase Epsilon**: When you have noisy data and want to create a more robust model that's less sensitive to individual data points.\n",
    "- **Increase Gamma**: When you suspect the relationship between features and target variable is highly nonlinear, and you want the decision boundary to be more flexible.\n",
    "- **Choose Appropriate Kernel**: Choose the kernel function based on the data's characteristics; for instance, RBF for complex nonlinear relationships and linear for linear relationships.\n",
    "\n",
    "It's crucial to perform cross-validation and grid search to find the optimal combination of parameter values for your SVR model, as the effectiveness of these parameters can vary greatly depending on the dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3aea0-78e3-4181-a406-ee3168d12cd0",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataseg\n",
    "- Split the dataset into training and testing sets\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- hse the trained classifier to predict the labels of the testing datW\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "- Train the tuned classifier on the entire dataseg\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9a3cb1-89b4-4912-8906-0d61446c6584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Accuracy: 0.9666666666666667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.89      0.94         9\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data - Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Define hyperparameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc = SVC(**best_params)\n",
    "tuned_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc, 'tuned_svc_classifier.pkl')\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = tuned_svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
